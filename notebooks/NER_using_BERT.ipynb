{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjWKB8K2YnVx",
        "outputId": "6533b55e-668c-4cfc-fecf-e53f33d90ad4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (4.25.1)\n",
            "Requirement already satisfied: filelock in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from transformers) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.10.0)\n",
            "Requirement already satisfied: colorama in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Installing transformers library\n",
        "\n",
        "%pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNr1lt66Z3jF",
        "outputId": "36080540-96fe-475b-e788-f0713a9c5a3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
            "Requirement already satisfied: torch in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (2.2.2+cpu)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.17.2-cp38-cp38-win_amd64.whl.metadata (6.6 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.2.2-cp38-cp38-win_amd64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: filelock in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from torch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: numpy in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from torchvision) (1.24.4)\n",
            "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
            "  Downloading pillow-10.3.0-cp38-cp38-win_amd64.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in d:\\data science\\mlops_projects\\name-entity-recognition\\venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Downloading torchvision-0.17.2-cp38-cp38-win_amd64.whl (1.2 MB)\n",
            "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.0/1.2 MB 991.0 kB/s eta 0:00:02\n",
            "   ----- ---------------------------------- 0.2/1.2 MB 1.8 MB/s eta 0:00:01\n",
            "   ---------- ----------------------------- 0.3/1.2 MB 2.4 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 0.5/1.2 MB 3.2 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 0.6/1.2 MB 3.0 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 0.9/1.2 MB 3.3 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 1.0/1.2 MB 3.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  1.2/1.2 MB 3.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.2/1.2 MB 3.2 MB/s eta 0:00:00\n",
            "Downloading torchaudio-2.2.2-cp38-cp38-win_amd64.whl (2.4 MB)\n",
            "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.1/2.4 MB 2.2 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 0.2/2.4 MB 1.6 MB/s eta 0:00:02\n",
            "   ------ --------------------------------- 0.4/2.4 MB 2.9 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 0.5/2.4 MB 2.9 MB/s eta 0:00:01\n",
            "   ---------- ----------------------------- 0.6/2.4 MB 3.0 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 0.8/2.4 MB 3.0 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 0.9/2.4 MB 3.0 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 1.0/2.4 MB 2.9 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 1.2/2.4 MB 3.0 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 1.4/2.4 MB 3.1 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 1.6/2.4 MB 3.0 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 1.7/2.4 MB 3.0 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 1.8/2.4 MB 3.0 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 2.0/2.4 MB 3.0 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 2.1/2.4 MB 3.0 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 2.1/2.4 MB 2.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  2.4/2.4 MB 3.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.4/2.4 MB 3.0 MB/s eta 0:00:00\n",
            "Downloading pillow-10.3.0-cp38-cp38-win_amd64.whl (2.5 MB)\n",
            "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 0.2/2.5 MB 4.0 MB/s eta 0:00:01\n",
            "   ------- -------------------------------- 0.5/2.5 MB 4.3 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 0.6/2.5 MB 3.6 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 0.8/2.5 MB 3.2 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 1.1/2.5 MB 3.6 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 1.2/2.5 MB 3.6 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 1.5/2.5 MB 3.7 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 1.6/2.5 MB 3.5 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 1.8/2.5 MB 3.6 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 1.9/2.5 MB 3.5 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 2.3/2.5 MB 3.7 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 2.4/2.5 MB 3.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.5/2.5 MB 3.6 MB/s eta 0:00:00\n",
            "Installing collected packages: pillow, torchvision, torchaudio\n",
            "Successfully installed pillow-10.3.0 torchaudio-2.2.2 torchvision-0.17.2\n"
          ]
        }
      ],
      "source": [
        "# Installing PyTorch\n",
        "\n",
        "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4v_1NSK8YvaW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\DATA SCIENCE\\MLOPS_Projects\\Name-Entity-Recognition\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries \n",
        "\n",
        "import pandas as pd\n",
        "import torch \n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.optim import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Mix7jNc6Y6tV",
        "outputId": "bed9c4a8-7056-450c-cc0d-42eacac73a53"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>47954</th>\n",
              "      <td>Opposition leader Mir Hossein Mousavi has said...</td>\n",
              "      <td>O O O B-per I-per O O O O O O O O O O O O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47955</th>\n",
              "      <td>On Thursday , Iranian state media published a ...</td>\n",
              "      <td>O B-tim O B-gpe O O O O O O O O B-org I-org O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47956</th>\n",
              "      <td>Following Iran 's disputed June 12 elections ,...</td>\n",
              "      <td>O B-geo O O B-tim I-tim O O O O O O O O O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47957</th>\n",
              "      <td>Since then , authorities have held public tria...</td>\n",
              "      <td>O O O O O O O O O O O O O O O O O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47958</th>\n",
              "      <td>The United Nations is praising the use of mili...</td>\n",
              "      <td>O B-org I-org O O O O O O O O O O O O O O B-ti...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  \\\n",
              "47954  Opposition leader Mir Hossein Mousavi has said...   \n",
              "47955  On Thursday , Iranian state media published a ...   \n",
              "47956  Following Iran 's disputed June 12 elections ,...   \n",
              "47957  Since then , authorities have held public tria...   \n",
              "47958  The United Nations is praising the use of mili...   \n",
              "\n",
              "                                                  labels  \n",
              "47954  O O O B-per I-per O O O O O O O O O O O O O O ...  \n",
              "47955  O B-tim O B-gpe O O O O O O O O B-org I-org O ...  \n",
              "47956  O B-geo O O B-tim I-tim O O O O O O O O O O O ...  \n",
              "47957          O O O O O O O O O O O O O O O O O O O O O  \n",
              "47958  O B-org I-org O O O O O O O O O O O O O O B-ti...  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Reading csv data\n",
        "\n",
        "df = pd.read_csv(r'D:\\DATA SCIENCE\\MLOPS_Projects\\Name-Entity-Recognition\\data\\ner.csv')\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Vf0tBzMXZcWx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\DATA SCIENCE\\MLOPS_Projects\\Name-Entity-Recognition\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vinit\\.cache\\huggingface\\hub\\models--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        }
      ],
      "source": [
        "# Creating tokenizer intsance\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gM0zKzshZe4Q"
      },
      "outputs": [],
      "source": [
        "# Creating Dataset class\n",
        "\n",
        "label_all_tokens = False\n",
        "\n",
        "def align_label(texts, labels):\n",
        "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
        "\n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "\n",
        "    for word_idx in word_ids:\n",
        "\n",
        "        if word_idx is None:\n",
        "            label_ids.append(-100)\n",
        "\n",
        "        elif word_idx != previous_word_idx:\n",
        "            try:\n",
        "                label_ids.append(labels_to_ids[labels[word_idx]])\n",
        "            except:\n",
        "                label_ids.append(-100)\n",
        "        else:\n",
        "            try:\n",
        "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
        "            except:\n",
        "                label_ids.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    return label_ids\n",
        "\n",
        "class DataSequence(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        lb = [i.split() for i in df['labels'].values.tolist()]\n",
        "        txt = df['text'].values.tolist()\n",
        "        self.texts = [tokenizer(str(i),\n",
        "                               padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for i in txt]\n",
        "        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_data(self, idx):\n",
        "\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "\n",
        "        return torch.LongTensor(self.labels[idx])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_data = self.get_batch_data(idx)\n",
        "        batch_labels = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ffRKNjwgZjYi"
      },
      "outputs": [],
      "source": [
        "# splitting the data into train, test and validation\n",
        "# Defining Unique labels\n",
        "\n",
        "df = df[0:2000]\n",
        "\n",
        "labels = [i.split() for i in df['labels'].values.tolist()]\n",
        "unique_labels = set()\n",
        "\n",
        "for lb in labels:\n",
        "        [unique_labels.add(i) for i in lb if i not in unique_labels]\n",
        "labels_to_ids = {k: v for v, k in enumerate(unique_labels)}\n",
        "ids_to_labels = {v: k for v, k in enumerate(unique_labels)}\n",
        "\n",
        "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42),\n",
        "                            [int(.8 * len(df)), int(.9 * len(df))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uFxm0JMpZsfK"
      },
      "outputs": [],
      "source": [
        "# Creating Model class\n",
        "\n",
        "class BertModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(BertModel, self).__init__()\n",
        "\n",
        "        self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(unique_labels))\n",
        "\n",
        "    def forward(self, input_id, mask, label):\n",
        "\n",
        "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68hQyuwxZvor",
        "outputId": "5d1abbf6-d81b-4198-c1af-b7b0772eb5d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error while downloading from https://cdn-lfs.huggingface.co/bert-base-cased/d6992b8cd27d7a132eafce6a8210272329a371b1c762d453588795dd3835593e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1712469078&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMjQ2OTA3OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9iZXJ0LWJhc2UtY2FzZWQvZDY5OTJiOGNkMjdkN2ExMzJlYWZjZTZhODIxMDI3MjMyOWEzNzFiMWM3NjJkNDUzNTg4Nzk1ZGQzODM1NTkzZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=WXpClIwXOkbQoT8gZSdvrDh5PtOwfQPXqszJVH5cUKLY-jmKLGhGqTS%7EOHQkjb1m6M9Tx29hV3zaDjwVDNYRfBBln9CCE6A37WK2AMpDU5IRRtO05bITK3dgwjr0HoMqOxJNJrJZcBPMvBLGD36un3fiHTlUptmEVSGVXVa5xx6di532Q7BwWfqefsyriTpc1Vqs4ewDDUN8UhEcu3dCQlpGM17wydAilQ3atdx0s371K-VALWL%7Edec8NknapvutOo7AuA5lUH3AGSDoo3OMR2vQD81L2mjowOguEG-AmX84Zl4v2gmmu-b8Ws2P7t4jbEI4dL%7EYOfR8o8njC%7Eo8Mg__&Key-Pair-Id=KVTP0A1DKRTAX: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
            "Trying to resume download...\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0%|          | 0/800 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "# Training the model [Transfer learning]\n",
        "\n",
        "def train_loop(model, df_train, df_val):\n",
        "\n",
        "    train_dataset = DataSequence(df_train)\n",
        "    val_dataset = DataSequence(df_val)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=BATCH_SIZE)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    best_acc = 0\n",
        "    best_loss = 1000\n",
        "\n",
        "    for epoch_num in range(EPOCHS):\n",
        "\n",
        "        total_acc_train = 0\n",
        "        total_loss_train = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for train_data, train_label in tqdm(train_dataloader):\n",
        "\n",
        "            train_label = train_label.to(device)\n",
        "            mask = train_data['attention_mask'].squeeze(1).to(device)\n",
        "            input_id = train_data['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss, logits = model(input_id, mask, train_label)\n",
        "\n",
        "            for i in range(logits.shape[0]):\n",
        "\n",
        "              logits_clean = logits[i][train_label[i] != -100]\n",
        "              label_clean = train_label[i][train_label[i] != -100]\n",
        "\n",
        "              predictions = logits_clean.argmax(dim=1)\n",
        "              acc = (predictions == label_clean).float().mean()\n",
        "              total_acc_train += acc\n",
        "              total_loss_train += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        total_acc_val = 0\n",
        "        total_loss_val = 0\n",
        "\n",
        "        for val_data, val_label in val_dataloader:\n",
        "\n",
        "            val_label = val_label.to(device)\n",
        "            mask = val_data['attention_mask'].squeeze(1).to(device)\n",
        "            input_id = val_data['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            loss, logits = model(input_id, mask, val_label)\n",
        "\n",
        "            for i in range(logits.shape[0]):\n",
        "\n",
        "              logits_clean = logits[i][val_label[i] != -100]\n",
        "              label_clean = val_label[i][val_label[i] != -100]\n",
        "\n",
        "              predictions = logits_clean.argmax(dim=1)\n",
        "              acc = (predictions == label_clean).float().mean()\n",
        "              total_acc_val += acc\n",
        "              total_loss_val += loss.item()\n",
        "\n",
        "        val_accuracy = total_acc_val / len(df_val)\n",
        "        val_loss = total_loss_val / len(df_val)\n",
        "\n",
        "        print(\n",
        "            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n",
        "\n",
        "LEARNING_RATE = 5e-3\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "model = BertModel()\n",
        "train_loop(model, df_train, df_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flo7qqd6ZzKS",
        "outputId": "1c27a91e-f4be-402d-b462-bcf19f11250b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy:  0.946\n"
          ]
        }
      ],
      "source": [
        "# Evaluate model\n",
        "\n",
        "def evaluate(model, df_test):\n",
        "\n",
        "    test_dataset = DataSequence(df_test)\n",
        "\n",
        "    test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    total_acc_test = 0.0\n",
        "\n",
        "    for test_data, test_label in test_dataloader:\n",
        "\n",
        "            test_label = test_label.to(device)\n",
        "            mask = test_data['attention_mask'].squeeze(1).to(device)\n",
        "\n",
        "            input_id = test_data['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            loss, logits = model(input_id, mask, test_label)\n",
        "\n",
        "            for i in range(logits.shape[0]):\n",
        "\n",
        "              logits_clean = logits[i][test_label[i] != -100]\n",
        "              label_clean = test_label[i][test_label[i] != -100]\n",
        "\n",
        "              predictions = logits_clean.argmax(dim=1)\n",
        "              acc = (predictions == label_clean).float().mean()\n",
        "              total_acc_test += acc\n",
        "\n",
        "    val_accuracy = total_acc_test / len(df_test)\n",
        "    print(f'Test Accuracy: {total_acc_test / len(df_test): .3f}')\n",
        "\n",
        "\n",
        "evaluate(model, df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFaET41rcmxA",
        "outputId": "b6a1b85e-1dc0-447b-b3fc-176c427982b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bill Gates is the founder of Microsoft\n",
            "['B-per', 'I-per', 'O', 'O', 'O', 'O', 'B-org']\n"
          ]
        }
      ],
      "source": [
        "# Predicting a sentence\n",
        "\n",
        "def align_word_ids(texts):\n",
        "  \n",
        "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
        "\n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "\n",
        "    for word_idx in word_ids:\n",
        "\n",
        "        if word_idx is None:\n",
        "            label_ids.append(-100)\n",
        "\n",
        "        elif word_idx != previous_word_idx:\n",
        "            try:\n",
        "                label_ids.append(1)\n",
        "            except:\n",
        "                label_ids.append(-100)\n",
        "        else:\n",
        "            try:\n",
        "                label_ids.append(1 if label_all_tokens else -100)\n",
        "            except:\n",
        "                label_ids.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    return label_ids\n",
        "\n",
        "\n",
        "def evaluate_one_text(model, sentence):\n",
        "\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    mask = text['attention_mask'].to(device)\n",
        "    input_id = text['input_ids'].to(device)\n",
        "    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n",
        "\n",
        "    logits = model(input_id, mask, None)\n",
        "    logits_clean = logits[0][label_ids != -100]\n",
        "\n",
        "    predictions = logits_clean.argmax(dim=1).tolist()\n",
        "    prediction_label = [ids_to_labels[i] for i in predictions]\n",
        "    print(sentence)\n",
        "    print(prediction_label)\n",
        "            \n",
        "evaluate_one_text(model, 'Bill Gates is the founder of Microsoft')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4PyFZGgctt0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    },
    "vscode": {
      "interpreter": {
        "hash": "e87c311b38e3867b44965d1fc544340be30d9bb75864c35cc64a91cfbbf57e74"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
